{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qqq llama-index llama-hub langchain accelerate==0.21.0 bitsandbytes==0.40.2 transformers sentence-transformers\n",
    "%pip install llama-index-llms-huggingface\n",
    "%pip install --upgrade openai\n",
    "%pip install llama-index-embeddings-langchain\n",
    "%pip install pydrive2\n",
    "%pip install dropbox\n",
    "%pip install boto3\n",
    "#%pip install ctransformers[cuda]\n",
    "%pip install llama-index-llms-llama-cpp\n",
    "%pip install matplotlib\n",
    "%pip install chainlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "import json\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "# transformers\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# llama_index\n",
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core import SimpleDirectoryReader, download_loader, Document, VectorStoreIndex, ServiceContext\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "from llama_index.core.callbacks import CallbackManager, LlamaDebugHandler\n",
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "from IPython.display import Markdown, display, HTML\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "\n",
    "import os\n",
    "import chainlit as cl\n",
    "from langchain.llms import CTransformers\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain_community.llms import CTransformers\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "import io\n",
    "import os\n",
    "import tempfile\n",
    "from pydrive2.auth import GoogleAuth\n",
    "from pydrive2.drive import GoogleDrive\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "import dropbox\n",
    "from dropbox.exceptions import AuthError, ApiError\n",
    "import boto3\n",
    "from botocore.exceptions import NoCredentialsError, PartialCredentialsError, ClientError\n",
    "import tempfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_pipeline code\n",
    "\n",
    "#We can specify some directory locally to create SimpleDirectoryReader for example\n",
    "\n",
    "documents = SimpleDirectoryReader(\"path_to_folder\", filename_as_id=True).load_data()    \n",
    "\n",
    "#or we can get data from Cloud, I have prepared 3 code for 3 different sources(Google drive, AWS S3 Storage, and Dropbox)\n",
    "\n",
    "def download_files_from_google_drive(credentials_file, folder_id):\n",
    "    gauth = GoogleAuth()\n",
    "    gauth.LoadClientConfigFile(credentials_file)\n",
    "    gauth.LocalWebserverAuth()\n",
    "    drive = GoogleDrive(gauth)\n",
    "    query = f\"'{folder_id}' in parents and trashed=false\"\n",
    "    file_list = drive.ListFile({'q': query}).GetList()\n",
    "    \n",
    "    for file in file_list:\n",
    "        print(f'Downloading {file[\"title\"]} from Google Drive...')\n",
    "        temp_file_path = os.path.join(temp_dir, file['title'])\n",
    "        file_io = io.FileIO(temp_file_path, 'wb')\n",
    "        request = drive.auth.service.files().get_media(fileId=file['id'])\n",
    "        downloader = MediaIoBaseDownload(file_io, request)\n",
    "        done = False\n",
    "        while not done:\n",
    "            _, done = downloader.next_chunk()\n",
    "        file_io.close()\n",
    "\n",
    "    print(\"Completed downloading all files from Google Drive.\")\n",
    "\n",
    "def download_files_from_s3(region, access_key, secret_access_key, bucket_name):\n",
    "    s3 = boto3.resource(\n",
    "        's3',\n",
    "        aws_access_key_id=access_key,\n",
    "        aws_secret_access_key=secret_access_key,\n",
    "        region_name=region\n",
    "    )\n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "    for obj in bucket.objects.all():\n",
    "        file_path = os.path.join(temp_dir, obj.key)\n",
    "        bucket.download_file(obj.key, file_path)\n",
    "        print(f\"Downloaded {obj.key} from AWS S3 to {file_path}\")\n",
    "\n",
    "def download_files_from_dropbox(access_token, path=\"\"):\n",
    "    try:\n",
    "        dbx = dropbox.Dropbox(access_token)\n",
    "        files = dbx.files_list_folder(path).entries\n",
    "        for file_metadata in files:\n",
    "            if isinstance(file_metadata, dropbox.files.FileMetadata):\n",
    "                file_path = file_metadata.path_lower\n",
    "                temp_file_path = os.path.join(temp_dir, os.path.basename(file_path))\n",
    "                _, result = dbx.files_download(file_path)\n",
    "                with open(temp_file_path, 'wb') as f:\n",
    "                    f.write(result.content)\n",
    "                print(f\"Downloaded {file_path} from Dropbox to {temp_file_path}\")\n",
    "    except AuthError as e:\n",
    "        print('Error downloading from Dropbox: ' + str(e))\n",
    "    except ApiError as e:\n",
    "        print('API error: ' + str(e))\n",
    "\n",
    "# Google Drive\n",
    "google_credential_file = \"-\" #google cloud credentials\n",
    "google_folder_id = \"-\" #Folder Id to specific folder\n",
    "\n",
    "# Dropbox\n",
    "dropbox_access_token = \"-\" #dropbox api token\n",
    "\n",
    "# AWS S3 credientials and ids\n",
    "aws_region = \"-\"\n",
    "aws_access_key_id = \"-\"\n",
    "aws_secret_access_key =\"-\"\n",
    "aws_bucket_name = \"-\"\n",
    "\n",
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    \n",
    "    #download_files_from_google_drive(google_credential_file, google_folder_id)    \n",
    "\n",
    "    #download_files_from_dropbox(dropbox_access_token)    \n",
    "\n",
    "    #download_files_from_s3(aws_region, aws_access_key_id, aws_secret_access_key, aws_bucket_name)\n",
    "\n",
    "    documents = SimpleDirectoryReader(temp_dir, filename_as_id=True).load_data()    \n",
    "\n",
    "    print(f\"All files are temporarily stored in {temp_dir}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#Rag code \n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "def messages_to_prompt(messages):\n",
    "  prompt = \"\"\"\n",
    "        You are a business chatbot designed to assist users with their business-related tasks. Use the information from the DOCUMENTS section to provide accurate answers but act as if you knew this information innately.\n",
    "\n",
    "        DOCUMENTS:\n",
    "        {documents}\n",
    "\n",
    "        Please answer questions based solely on the data I have provided. If the information is not present in the documents, please respond with 'I don't have enough information to answer that.' Avoid making assumptions or providing speculative answers. Please adhere strictly to the data provided.\n",
    "\n",
    "        When answering questions, you should:\n",
    "        \n",
    "        Rely solely on the information present in the documents.\n",
    "        Avoid using external knowledge or making assumptions.\n",
    "        Respond with 'I don't have enough information to answer that.' if the data is not in the documents.\n",
    "        Provide clear, non-speculative answers.\n",
    "        Aim to answer to the best of your ability based on the data provided.\n",
    "        If unsure, simply state, \"I don't know.\" For any queries outside of the DOCUMENTs section, your response should be \"I do not know about this.\"\n",
    "\n",
    "        Your primary objectives are:\n",
    "\n",
    "        Answer Questions about the Provided Documents:\n",
    "          - Your role is to provide accurate and relevant information in response to user queries about the documents provided.\n",
    "          - If a user asks a question pertaining to a specific detail in the documents, your task is to extract the relevant information and provide a clear answer.\n",
    "          - For example, if a user asks about the ownership structure or financial status of the company mentioned in the documents, you should respond with precise details.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "  for message in messages:\n",
    "    if message.role == 'system':\n",
    "      prompt += f\"<|system|>\\n{message.content}</s>\\n\"\n",
    "    elif message.role == 'user':\n",
    "      prompt += f\"<|user|>\\n{message.content}</s>\\n\"\n",
    "    elif message.role == 'assistant':\n",
    "      prompt += f\"<|assistant|>\\n{message.content}</s>\\n\"\n",
    "\n",
    "  if not prompt.startswith(\"<|system|>\\n\"):\n",
    "    prompt = \"<|system|>\\n</s>\\n\" + prompt\n",
    "\n",
    "  # add final assistant prompt\n",
    "  prompt = prompt + \"<|assistant|>\\n\"\n",
    "\n",
    "  return prompt\n",
    "\n",
    "\n",
    "\n",
    "llm = LlamaCPP(\n",
    "    model_url='https://huggingface.co/TheBloke/zephyr-7B-alpha-GGUF/resolve/main/zephyr-7b-alpha.Q5_K_M.gguf',\n",
    "    model_path=None,\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=500,\n",
    "    context_window=3900,\n",
    "    generate_kwargs={ \"top_k\": 50, \"top_p\": 0.95},\n",
    "    model_kwargs={\"n_gpu_layers\": -1},\n",
    "    messages_to_prompt=messages_to_prompt,\n",
    "    verbose=True,\n",
    "\n",
    ") \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = HuggingFaceEmbeddings(model_name=\"thenlper/gte-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the LlamaDebugHandler to print the trace of the sub questions\n",
    "llama_debug = LlamaDebugHandler(print_trace_on_end=True)\n",
    "callback_manager = CallbackManager([llama_debug])\n",
    "\n",
    "# ServiceContext\n",
    "service_context = ServiceContext.from_defaults(llm=llm,\n",
    "                                               embed_model=embed_model,\n",
    "                                               callback_manager=callback_manager,\n",
    "                                               chunk_size_limit=512\n",
    "                                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=\"storage\")\n",
    "    index = load_index_from_storage(storage_context, index_id=\"vector_index\")\n",
    "except:\n",
    "    index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "    index.storage_context.persist(\"./storage\")\n",
    "#print('persisting to disk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_input = \"who is george bush?\" #Anything unrelated\n",
    "response = index.as_query_engine().query(query_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
